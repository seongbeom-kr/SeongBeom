#deep_learning 

> [!note]
> 신경망을 학습할 때는 최적화에 좋은 위치에서 출발하도록 초기화를 잘하는 것과 더불어, 최적해로 가는 길을 잘 찾을 수 있도록 **정규화`Regularization`** 하는 것이 중요함

정규화는 최적화 과정에서 최잭해를 잘 찾도록 정보를 추가하는 기법

정규화는 최적해가 어떤 공간에 있는지 알려주어 빠르게 찾을 수 있도록 하거나, 손실 함수를 부드럽게 만들어 최적해로 가는 길을 잘 닦아주기도하고, 최적해 주변을 평평하게 만들어서 새로운 데이터에 대해서도 모델이 좋은 성능을 갖도록 만들어주기도 한함

뿐만 아니라 모델과 데이터에 확률적 성질을 부여해서 비슷하지만 조금씩 다른 다양한 상황에서 학습하는 효과를 줄 수 있음 → 손실함수가 더 넓은 범위에서 세밀하게 표현되므로 정확한 해를 찾을 수 있음

# 1. 일반화 오류

<aside> 💡 모델의 성능이 좋다는 말은 일반적으로 일반화가 잘 되었다는 의미이다. 그럼 일반화란 무엇인가?

</aside>

**일반화** : 훈련 데이터가 아닌 새로운 데이터에 대해 모델이 예측을 얼마나 잘하는 지를 가르킴

모델의 훈련 성능과 검증/테스트 성능의 차를 일반화 오류라고 하며, 일반화 오류가 적을수록 일반화가 잘 된 모델임

검증/테스트 성능은 훈련 성능보다 낮을 수밖에 없지만, 두 성능의 차이가 작아야 모델이 훈련 데이터에 과적합되지 않고, 새로운 데이터에 대해 일반화를 잘하는 모델이 된다.

그래서 정규화는 일반화를 잘하는 모델을 만드는 기법이라고 함

신경망은 모델이 크고 복잡하기 때문에 파라미터 공간이 크고 학습 데이터가 많이 필요한데 이러한 이유로 과적합 되기 쉬우므로 신경망을 학습할 때는 반드시 정규화를 적용해야하며, 여러 정규화 기법을 조합해서 성능을 높여줘야 한다.

# 2. 정규화 접근 방식

<aside> 💡 정규화의 정의가 포괄적인 만큼 정규화 기법도 다양하지만, 기본적인 접근 방법은 다음과 같이 몇가지로 정리할 수 있음

</aside>

## 01. 모델을 최대한 단순하게 만든다.

단순한 모델은 복잡한 모델보다 파라미터 수가 적어서 과적합이 덜 생긴김

Ex) 학습과정에서 필요한 가중치만 남기고 필요하지 않은 가중치는 0이 되도록 만들면 과적합을 막을 수 있음 →**`L1`**

## 02. 사전 지식을 표현해서 최적해를 빠르게 찾도록 한다.

사전지식을 표현하는 방법은 다양하며, 그 중 한 예시가 데이터나 모델에 대한 사전 분포를 이용해서 정확하고 빠르게 해를 찾는 방법 → **`가중치 감소`**는 가중치의 사전 분포를 손실함수의 일부 항으로 표현해서 가중치의 크기를 조절하는 정규화 기법

## 03. 확률적 성질을 추가한다.

데이터 또는 모델, 훈련 기법 등에 확률 성질을 부여하여 조금씩 변화된 형태로 데이터를 처리함으로써 다양한 상황에서 학습하는 효과를 줄 수 있음

이럴 경우 손실함수는 풍부한 데이터를 이용해서 넓은 범위에서 세밀하게 표현되므로 정확한 해를 찾을 수 있고 모델이 잡음에 대해 민감하게 반응하지 않음 → **`데이터 증강, 잡음 주입, 드롭아웃`**

## 04. 여러 가설을 고려하여 예측한다.

하나의 모델로 예측하지 않고 여러 모델로 동시에 예측해서 그 결과에 따라 최종 예측하는 방식

이 경우 하나의 모델이 가질 수 있는 편향을 제거하여 오차를 최소화하고 공정하게 예측할 수 있음 → 앙상블 중 **`배깅`**

# 3. 배치 정규화

<aside> 💡 신경망 학습이 어려운 이유 중 하나는 계층을 지날 대마다 데이터 분포가 보이지 않는 요인에 의해 조금씩 왜곡되기 때문이다. 이를 막으려면 가중치 초기화를 잘해야 하고, 학습률도 작게 사용해야 하는데 이 경우 학습 속도가 느려지는 문제가 있음

</aside>

## 01. 내부 공변량 변화

데이터의 분포가 보이지 않는 요인에 의해 왜곡되는 현상을 내부 공변량 변화(`Interal covariate shift`)라 함

분포를 결정하는 보이지 않는 요인을 내부 공변량이라 하며, 내부 공변량이 바뀌면 각 계측의 데이터 분포가 원래 분포에서 조금씩 멀어진다. 그 결과 하위 계층의 작은 변화가 상위 계층으로 갈수록 큰 영향을 미치게 됨

## 02. 배치 정규화 단계

배치 정규화`(batch regularization)` : 데이터가 계층을 지날 때마다 매번 정규화해서 내부 공변량 변화를 없애는 방법

배치 정규화가 기존 데이터 정규화 방식과 다른 점은 모델의 계층 형태로 데이터 정규화를 실행한다는 점이다

따라서 배치 정규화를 하면 모델이 실행될 때마다 해당 계층에서 매번 데이터 정규화가 일어나고, 전체 데이터에 대해 정규화하지 않고, 미니배체에 대해 정규화한다

### 표준 가우시안 분포로 정규화

입력 데이터가 존재할 때, 배치 정규화는 차원별로 평균과 분산을 구해서 표준 가우시안 분포로 정규화한다.

표준 가우시안 분포로 정규화하므로 데이터의 크기가 작아지면서 내부 공변량의 변화도 작게 만들 수 있음

배치 정규화를 모든 계층에 적용하면 데이터가 계층을 지날 때마다 표준 가우시안 분포로 바뀌고 그에 따라 내부 공변량의 변화를 최소화할 수 있다. 원리적으로는 계층을 지나면서 생기는 데이터 오차의 크기를 줄임으로써 누적오차도 작게 만드는 작업을 하는 것

### 원래 분포로 복구

표준 가우시안 분포로 정규화하면 모델이 표현하려던 비선형서을 제대로 표현할 수 없는 문제가 생김

활성함수가 시그모이드라면 표준 가우시안 분포로 정규화된 데이터는 시그모이드 함수의 가운데 부분인 선형 영역을 통과하게 되므로 비선형성이 사라짐

활성함수가 ReLU인 경우에도 문제가 되는데, 표준 가우시안 분포로 정규화된 데이터의 절반은 음수고 나머지는 양수이므로 절반의 데이터의 출력이 0이되어 뉴런의 절반이 죽는 ReLU가 되면 정상적인 학습이 이루어지지않는다.

따라서 배치 정규화를 하면서 모델의 비선형성을 잘 표현하려면 데이터를 표준 가우시안 분포로 정규화한 뒤 다시 원래 데이터의분포로 복구해야함 → $y^{k} = r^k\hat x^k + \beta^k$ 로 복구 가능

하지만 여기서 평균과 표준편차를 어떻게 구할 것인가에 대한 문제가 생김

이상적으로는 미니배치에 대한 평균과 표준편차아 원래 데이터 분포를 표현한다면 이 값들을 이용해서 바로 복구 가능하지만, 실제 미니배치에 대한 평균과 표준편차가 원래 데이터 분포를 표현하지 못함 → 평균과 표준편차는 모델의 학습 과정에서 따로 구해야함

## 03. 배치 정규화 알고리즘
미니배치의 평균과 분산을 구해서 표준 가우시안 분포로 정규화를 수행한다. 그리고 다시 학습된 평균과 표준편차를 이용해서 원래 분포로 복구한다.

학습 단계에서는 미니배치 단위의 평균과 분산으로 정규화를 수행하지만, 테스트 단계에서는 전체 데이터의 평균과 분산으로 정규화해야한다.

### 배치 정규화 수행 위치
활성 함수 전과 후 둘다 수행한 다음 성능비교를 진행한다.

## 04. 이미지 정규화 기법
배치 정규화를 이미지에 적용할 때는 채널 단위로 정규화를 수행한다. 이미지의 경우 배치정규화 이외의 응용에 따라 좀 더 세분화된 정규화 방식을 사용한다.
- RNN : 계층 정규화
- 스타일 변화, GAN : 인스턴스 정규화
- 미니배치 크기가 작을 때 : 그룹 정규화

## 05. 우수성
배치 정규화를 하면 내부 공변량 변화가 최소화되므로 그라디언트의 흐름이 원활해지고, 그에 따라 학습이 안정적으로 진행된다.

지속적으로 데이터 분포를 유지하기에 초기화 방법에 대한 의존도가 낮아지고, 높은 학습률을 사용해도 된다.

미니배치 단위로 정규화하므로 