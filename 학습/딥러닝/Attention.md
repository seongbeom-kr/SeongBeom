#deep_learning 

> [!note]+ 개요
> LSTM과 GRU와 같은 순환신경망 구조가 등장하면서 장기 의존성 문제와 그라디언트 폭발 문제를 조금 완화했지만 구조적으로 해결될 수 없다.
> 
> 여전히 더 많은 문제에서 긴 Context를 다뤄야 하므로, 다른 구조를 통해 순차 데이터를 다루려는 시도는 계속 진행되고 있다.
> 

# 5.1 Attention
_어텐션_ 이란 뇌가 가장 중요하고 연관성있는 정보에 집중하는 것을 말한다.

### 언어 모델의 어텐션 활용
언어 모델에 어텐션을 적용한다면 문장의 특정단어를 볼 대 문장 내의 다른 단어와의 연관성을 _어텐션값(attention value)_ 으로 사용한다. 

### 어텐션 계산을 위한 쿼리와 (키, 값) 목록
어텐션을 계산하려면 키와 값으로 이뤄진 $(k_i, v_i), i = 1,2,3,...,N$  목록과 $쿼리 q$가 정의되어야 한다.
쿼리 q를 기준으로 각 키 $k_i$와의 연관 정도를 계산하며, 그에 비례해서 키에 대응되는 값 $v_i$를 어텐션 값으로 사
# Transformer
S2S모델
인코더의 스택
디코더의 스택
## self attention
디코더와 인코더의 관계를 보는 것이 아닌
인풋의 단어들끼리의 관계를 보는 방법

-> 
????????????????????????????

