#deep_learning 

> [!note]+ 개요
> LSTM과 GRU와 같은 순환신경망 구조가 등장하면서 장기 의존성 문제와 그라디언트 폭발 문제를 조금 완화했지만 구조적으로 해결될 수 없다.
> 
> 여전히 더 많은 문제에서 긴 Context를 다뤄야 하므로, 다른 구조를 통해 순차 데이터를 다루려는 시도는 계속 진행되고 있다.
> 

# 5.1 Attention
_어텐션_ 이란 뇌가 가장 중요하고 연관성있는 정보에 집중하는 것을 말한다.

### 언어 모델의 어텐션 활용
언어 모델에 어텐션을 적용한다면 문장의 특정단어를 볼 대 문장 내의 다른 단어와의 연관성을 _어텐션값(attention value)_ 으로 사용한다. 

### 어텐션 계산을 위한 쿼리와 (키, 값) 목록
어텐션을 계산하려면 키와 값으로 이뤄진 $(k_i, v_i), i = 1,2,3,...,N$  목록과 $쿼리 q$가 정의되어야 한다.
쿼리 q를 기준으로 각 키 $k_i$와의 연관 정도를 계산하며, 그에 비례해서 키에 대응되는 값 $v_i$를 어텐션 값으로 사용한다. 따라서 키는 쿼리와의 연관정도를 계산할 대상이고, 값은 연관정도에 따라 사용할 데이터이다. 어텐션을 다음과 같이 어텐션을 반환하는 함수로 생각하다.
$$어텐션 = attention(q,k,r)$$

### 어텐션 계산 순서
먼저 $q$와 $(k_i, v_i)$ 목록의 각 키 $k_i$ 와 어텐션 점수를 계산한다. 어텐션 점수는 q와 k_i 의 연관 정도를 의미하며 다양한 함수로 표현될 수 있다. 계산된 점수와 값 v_i를 곱해서 모두 합산하거나 가장 큰 어텐션 점수와 대응되는 v_i를 곱해서 최종 어텐션으로 사용한다.

### 어텐션 점수와 종류
어텐션 점수는 쿼리와 키 사이에 연관성 또는 유사도를 나타내므로 보통 내적 또는 내적을 변형한 방법으로 계산한다. 예를 들어 내적을 사용한다면 $score(q,k) = q^Tk$ 와 같은 식으로 계산하며 크기를 조정한 내적은 벡터 q, k의 크기가 달라져도 동일한 크기의 내적값을 얻기 위해 내적값을 q와 k의 벡터크기인 $n^{0.5}$으로 나눠서 정규화한다.
$$score(q,k) = {q^Tk \over n^{0.5}}$$

종종 어텐션 점수에 소프트맥스를 적용해서 확률로 변환하기도 한다.
$$softmax(socre(q,k))$$


### 셀프 어텐션
셀프 어텐션은 자기 자신을 구성하는 부분끼리 연관성을 찾고자 할 때 사용하는 어텐션 방법이다.

예를 들어 문장 내에 현재 단어와 연관성이 있는 단어를 찾거나 이미지에서 의미적으로 서로 연관성 있는 부분을 찾을 때 사용한다. 

### 하드어텐션과 소프트 어텐션
어텐션을 계산하는 방법에 따라 하드 어텐션과 소프트 어텐션을 구분하기도 한다.

하드어텐션은 가장 집중하는 정보를 선택하는 방식으로 어텐션 점수 중 최대값ㄱ

# Transformer
S2S모델
인코더의 스택
디코더의 스택
## self attention
디코더와 인코더의 관계를 보는 것이 아닌
인풋의 단어들끼리의 관계를 보는 방법

-> 
????????????????????????????

