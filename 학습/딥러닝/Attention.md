#deep_learning 

> [!note]+ 개요
> LSTM과 GRU와 같은 순환신경망 구조가 등장하면서 장기 의존성 문제와 그라디언트 폭발 문제를 조금 완화했지만 구조적으로 해결될 수 없다.
> 
> 여전히 더 많은 문제에서 긴 Context를 다뤄야 하므로, 다른 구조를 통해 순차 데이터를 다루려는 시도는 계속 진행되고 있다.
> 

# 5.1 Attention
_어텐션_ 이란 뇌가 가장 중요하고 연관성있는 정보에 집중하는 것을 말한다.

### 언어 모델의 어텐션 활용
언어 모델에 어텐션을 적용한다면 문장의 특정단어를 볼 대 문장 내의 다른 단어와의 연관성을 _어텐션값(attention value)_ 으로 사용한다. 

### 어텐션 계산을 위한 쿼리와 (키, 값) 목록
어텐션을 계산하려면 키와 값으로 이뤄진 $(k_i, v_i), i = 1,2,3,...,N$  목록과 $쿼리 q$가 정의되어야 한다.
쿼리 q를 기준으로 각 키 $k_i$와의 연관 정도를 계산하며, 그에 비례해서 키에 대응되는 값 $v_i$를 어텐션 값으로 사용한다. 따라서 키는 쿼리와의 연관정도를 계산할 대상이고, 값은 연관정도에 따라 사용할 데이터이다. 어텐션을 다음과 같이 어텐션을 반환하는 함수로 생각하다.
$$어텐션 = attention(q,k,r)$$

### 어텐션 계산 순서
먼저 $q$와 $(k_i, v_i)$ 목록의 각 키 $k_i$ 와 어텐션 점수를 계산한다. 어텐션 점수는 q와 k_i 의 연관 정도를 의미하며 다양한 함수로 표현될 수 있다. 계산된 점수와 값 v_i를 곱해서 모두 합산하거나 가장 큰 어텐션 점수와 대응되는 v_i를 곱해서 최종 어텐션으로 사용한다.

### 어텐션 점수와 종류
어텐션 점수는 쿼리와 키 사이에 연관성 또는 유사도를 나타내므로 보통 내적 또는 내적을 변형한 방법으로 계산한다. 예를 들어 내적을 사용한다면 scor
# Transformer
S2S모델
인코더의 스택
디코더의 스택
## self attention
디코더와 인코더의 관계를 보는 것이 아닌
인풋의 단어들끼리의 관계를 보는 방법

-> 
????????????????????????????

