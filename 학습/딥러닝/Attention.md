#deep_learning 

> [!note]+ 개요
> LSTM과 GRU와 같은 순환신경망 구조가 등장하면서 장기 의존성 문제와 그라디언트 폭발 문제를 조금 완화했지만 구조적으로 해결될 수 없다.
> 
> 여전히 더 많은 문제에서 긴 Context를 다뤄야 하므로, 다른 구조를 통해 순차 데이터를 다루려는 시도는 계속 진행되고 있다.
> 

# 5.1 
# Transformer
S2S모델
인코더의 스택
디코더의 스택
## self attention
디코더와 인코더의 관계를 보는 것이 아닌
인풋의 단어들끼리의 관계를 보는 방법

-> 
????????????????????????????

