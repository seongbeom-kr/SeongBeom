행렬과 머신러닝, 딥러닝


# 손실함수란?
![](https://i.imgur.com/JRJXMss.png)

오차 : 예측값과 실제값의 차이
비용함수 : 모든 오차를 최소화하기 위해 정의되는 함수

![](https://i.imgur.com/n5Hif8I.png)
$y = \alpha x + \beta$ 라고 가정할 때 우리는 오차를 최대한 줄이는 알파를 찾는게 목적임

이 알파를 찾기 위해서 많은 손실 함수를 정의하고 사용함

SSE, CEE ... Task와 데이터에 따라 손실함수를 정의하면 됨

## MAE
![](https://i.imgur.com/ckO5fld.png)
그래프로 그리면 다음과 같음

장점 : 학습된 정도를 파악할 수 있음

단점 : 오차가 발생해도 음수인지 양수인지 알 수 없음

## MSE
![](https://i.imgur.com/Lu04OZJ.png)
장점:

- 실제 정답에 대한 정답률의 오차뿐만 아니라 다른 오답에 대한 정답률의 오차도 포함하여 계산해준다.
- MAE와 달리 최적값에 가까워질수록 이동값이 다르게 변화하기 때문에 최적값에 수렴하기 용이하다.

단점:

- 값을 제곱하기 때문에 절댓값이 1미만인 값은 더 작아지고, 1보다 큰 값은 더 커지는 왜곡이 발생할 수 있다.
- 제곱하기 때문에 특이값의 영향을 많이 받는다.

## 경사하강법
MSE의 식

![](https://i.imgur.com/gPNesNY.png)

이걸 미분하면 됨


# 확률적 경사하강법

확률적 경사하강법이란 손실 함수의 곡면에서 **'경사가 가장 가파른 곳으로 내려가다 보면 언젠가 가장 낮은 지점에 도달한다.'** 는 가정으로 만들어 짐

## 고정된 학습률
학습률 : 최적화 할 때 한 걸음의 폭을 결정하는 스텝 크기를 말하며 학습 속도를 결정한다. 

확률적 경사하강법은 지정된 학습률을 사용하는 알고리즘이므로 경험적으로 학습률을 결정할 수 밖에 없음

### 학습률을 조정하지 않으면 생기는 일
- 매우 비효율적임
- 높으면 최적해로 수렴하지 못하거나 손실이 커지는 방향으로 점점 발산할 수 있음
- 낮으면 속도가 느림

![](https://i.imgur.com/1grZdTO.png)

일반적인 경우 큰 학습률을 사용해 최대한 빠르게 내려가고, 어느정도 내려가면 작은 폭으로 천천히 이동해서 최적해에 접근하는 법을 사용 -> 스케쥴러 사용

## 경사하강법을 통한 최적화 방법은 가장 좋은 방법은 아님
비볼록 함수에도 이야기가 달라짐
![](https://i.imgur.com/h9DHxcu.png)

어느 점에서 시작하느냐에 따라서도 좀 달라짐

saddle point 문제도 있음
![](https://i.imgur.com/wu2Hveg.png)

이런 문제를 해결하기 위해 다양한 최적화 방법이 있음
