---

---
#deep_learning #RNN 
> [!note]+
> 모든 종류의 신호를 하나의 네트워크에서 처리하는 생체 신경망과 달리 인공신경망은 데이터 구조에 따라 각기 다른 신경망으로 처리한다. 데이터에 특별한 구조가 없다면 순방향 신경망으로 처리하지만, 2D, 3D 공간 데이터를 다룰 때는 공간상에 존재하는 특징과 형체를 포착하기 위해서는 컨볼루션 신경망을 사용한다.
> 
> 이번에는 시간적 공간적 순서 관계가 있는 순차데이터를 다룬다. 순차데이터를 다루기 위해 순방향 신경망이 어떤 구조를 갖는지, 이런 구조로 인해 발생하는 문제점은 무엇인지, 그리고 이 문제를 해결하기 위해 어떤 방향으로 발전되고 있는지 살펴본다.


# 1. 기억을 갖는 신경망 모델 RNN




__순차 데이터__ : 시간적 공간적 순서 관계가 있는 데이터

순차데이터는 시공간의 순서 관계로 형성되는 _문맥_ 또는 _콘텍스트(context)_ 를 갖는다.

현재 데이터를 이해할 때 앞 뒤에 있는 데이터를 함께 살펴보면서 콘텍스트를 파악해야 현재 데이터의 역할을 이해할 수 있다.

`나는 사과를 갖고 싶다.`  이 문장을 예시로는 사과까지만 나타냈을 때는 먹는 사과인지 용서를 구하는 사과인지 알 수 없다. 그렇기 때문에 앞 뒤 주변 단어들을 살펴봐야 판단할 수 있다.

만약 여기서 위 문장을 순방향 신경망에 입력한다면 어떻게 될까?

순방향 신경망은 입력데이터의 크기가 고정되어있고, 한 번에 입력되는 단어의 순서 정보만 알 기 때문에, _입력 데이터의 크기를 넘어서는 문장_ 을 처리하기에는 __부적절__ 하다.

이러한 문제점을 해결하기 위해 처음 등장한 네트워크는 홉필드 네트워크이다.

## 1.1 연상 기억을 하는 홉필드 네트워크
홉필드 네트워크는 새로운 입력이 들어오면 특정 패턴으로 수렴하게 만들어 기억해둔 패턴을 연상하는 네트워크이다.

이때 양극화 기법을 사용하는데, 양극화는 값이 1에 조금이라도 가까우면 1이되고, -1에 조금이라도 가까우면 -이 되게 만들어준다.

연상하려는 패턴을 1과 -1로 된 벡터로 정의하고, 입력데이터가 특정패턴으로 양극화되도록 사전에 가중치 편향을 계산해둔다.

기억해둔 패턴을 연상하기 위해 입력데이터가 양극화될 때까지 출력을 입력으로 피드백하여 연산을 반복하다가, 같아지면 연산을 멈춘다.

## 1.2 기억을 전달하는 순환신경망
인공신경망이 데이터의 순서를 고려하는 콘텍스트를 만들려면 데이터의 순차구조를 인식할 수 있어야 하고, 데이터의 콘텍스트 범위가 넓더라도 처리할 수 있어야 한다. 이러한 점들을 고려하여 만든 인공신경망이 _순환신경망(Recurrent neural network)_ 이다.

### 순차구조를 인식하며 콘텍스트를 기억하는 모델구조
순방향 신경망이나 컨볼루션 신경망과는 달리 순환신경망은 데이터의 순차구조를 인식하기 위해 데이터를 시간 순서대로 하나씩 입력 받는다. 그리고 순서대로 입력받은 데이터의 콘텍스트를 만들기 위해 은닉 계층에 피드백을 연결을 가진다.


![](https://i.imgur.com/f5lnuUI.png)[왼쪽 : 은닉계층에 피드백 연결을 갖는 모델 구조, 오른쪽은 모델이 데이터를 처리하는 과정을 시간 순서에 따라 보여준다.]

기본적으로 모델은 입력계층, 은닉 계층, 출력 계층으로 이루어지며 은닉계층은 여러 계층이 될 수 있다. 하지만 일반적으로 순환신경망은 은닉계층을 깊게 쌓아도 성능이 크게 향상이 되지 않는 경우가 많기 때문에 보통 한 두 게층 이내로 쌓는다. 

각 시간단계에서 데이터가 처리되는 과정은 순방향 신경망과 동일하게 입력 계층, 은닉계층, 출력계층 순서대로 실행된다. 다만  한가지 차이점은 은닉계층에 피드백연결이 있기 때문에 은닉상태가 다음 단계로 전달된다는 점이다.

> 여기서 말하는 은닉상태는 은닉 계층의 출력을 말한다.

은닉상태는 _"시간 단계별로 입력된 데이터가 순차적으로 추상화되어 형성된 콘텍스트"_ 를 저장한다. 그리고 피드백 연결은 _시간의 흐름에 따라 콘텍스트를 기억하는 과정_ 으로 생각할 수 있다.

### 순환 연산 방식
순환 연산을 더 자세히 살펴보면,

초기 은닉상태 $h_0$은 영벡터라고 가정하고, 각 단계를 변수 $t (t >=1)$ 로 표현하면 다음과 같은 수서로 실행된다.
- 입력계층은 새로운 입력 데이터 $x_t$를 입력받는다.
- 은닉계층은 다음 순서로 실행된다.
	- 이전 상태 $h_{t-1}$와 입력데이터 $x_t$를 합쳐 $(h_{t-1}, x_t)$로 입력받는다.
	- 함수 $f_W(h_{t-1}, x_t)$를 실행해서 은닉상태 $h_t$를 출력한다.
	- 함수 $f_W$는 순환신경망의 종류에 따라 달라지며 W는 은닉계층의 가중칠르 나타낸다.
	- 은닉상태 $h_t$는 출력계층과 다음단계의 은닉계층에 전달된다.
- 출력계층은 은닉 상태 $h_t$를 입력받아서 뉴런 연산후 $y_t$를 출력한다.

### 기본 순환 신경망 모델
이때 함수 $f_W$가 다음과 같은 형태로 정의되는 신경망을 _기본 순환 신경망(Vanilla RNN)_ 이라 한다.

$$\
\begin{align*}
y_t &\quad = tanh(W_{hh}h_{t-1} + W_{xh}x_t) \\
&\quad = tanh(W_{hh}W_{xh}({h_{t-1}x_t })^T)\\

\end{align*} \ $$

기본 순환 신경망 모델에는 다음 그림과 같이 세 종류의 가중치 $W_{xh},W_{hh},W_{hy}$가 있다. $W_{xh}$는 입력계층과 은닉계층을 연결하는 가중치이고, 가중치 $W_{hh}$는 은닉 계층의 피드백에 대한 가중치이며 가중치 $W_{hy}$는 은닉계층과 출력계층을 연결하는 가중치다. 

순환 신경망의 가중치는 모든 시간 단계에서 공유된다.


![](https://i.imgur.com/QEMnuGv.png)

기본순환 신경망의 은닉계층은 입력 $h_{t-1}, x_t$와 가중치 $W= (W_{hh}, W_{xh})$를 가중합산한 뒤에 하이퍼볼릭 탄젠트를 실행한다.

## 1.3 순환신경망의 입력, 은닉 상태, 출력
각 층들이 무엇을 표현하는지에 대한 수식 확인

### 순환신경망의 입력 $(h_{t-1}, x_t)$와 은닉상태 $h_{t}$는 무엇을 표현할까?
$$\
\begin{align*}
h_1 = f_W(h_0, x_1)\\
h_2 = f_W(h_1, x_2)\\
h_3 = f_W(h_2, x_3)\\
\cdots \\
h_4 =f_W(h_3, x_4)\\
\end{align*} \ 
$$

$h_0, x_1$은 $x_1$이므로 은닉상태  $h_1$은  $(x_1)$이 추상화된 콘텍스트를 나타내고, $(h_1, x_2 )$는 ($x_1$의 콘텍스트, $x_2$)이므로 은닉상태 $h_2$는 ($x_1, x_2$)가 추상화된 콘텍스트를 형성한다 이후 t에 도입하면 일반화식이 완성된다.

### 순환신경망은 데이터의 순차 구조를 어떻게 포착할까?
은닉 계층은 '이전 상태, 새로운 입력'을 받아서 현재 상태를 매핑하는 함수이다.

$$h_t = f_W(f_{t-1}, x_t)$$



이 식은 점화식 형태로 h0까지 전개하면 다음과 같다.

![](https://i.imgur.com/KyrxGtK.png)

즉 은닉계층을 나타내는 함수 $f_W$는 `추상화된 순차구조를 한단계 확장된 추상화된 순차구조로 매핑하는 함수`이다.

### 순환신경망의 출력 $y_t$는 무엇으로 표현할까?
첫번째 단계의 출력은 입력 x_1이 주어졌을 때 y_1의 조건부 확률 $p(y_1|x_1)$으로 표현한다. 두번째 출력은 x_1이 h_1을 통해 h_2로 전달되었으므로 입력 x_1과 x_2가 주어졌을 때 y_2의 조건부 확률 p(y_2|x_1, x_2)를 표현한다.

이렇게 표현한식을 일반화 하여 t로 바꾼 다음, 다음과 같이 근사할 수 있다.
$$p(y_t|h_t)$$
은닉상태 $h_t$는 $(x_1, x_2, \cdots x,_t)$ 가 추상화되어있기 때문이다.

### 가중치 공유효과
순환 신경망은 가중치를 모든 단계에서 공유하는데 그에 따른 몇 가지 이점이 있다.

첫째, 순차 구조를 포착할 수 있다.
	데이터의 다양한 순차구조를 하나의 가중치로 학습하기에 특정 순차구조가 어느 위치에 있더라도 포착할 수 있다.

둘째 가변길이 데이터를 처리하기 쉽다.
	모든 단계가 같은 파라미터를 사용하므로 단계를 쉽게 추가할 수 있다. 따라서 가변길이 데이터를 유연하게 처리할 수 있다.

셋째, 파라미터수가 절약되고 정규화 효과가 생긴다.
	파라미터를 공유하면 파라미터 수가 줄고 비슷한 유형의 순차패턴에 대한 일반화를 잘 할 수 있다.
# 2. 순환 신경망의 주요 모델
> 순환 신경망은 입출력의 형태와 처리 방식에 따라 모델 구성이 다양하다.

- 입력은 순차열이지만 출력은 아니라면 _다대일_, 입출력이 모두 순차열이라면 _다대다_, 입력은 순차열이 아니지만 출력은 순차열인 경우는 _일대다_ 모델이 된다.
- 순방향과 역방향을 모두 살펴본다면 양방향 모델이라고 한다.
- 입출력의 길이가 다를때 입력을 처리하는 모델과 출력을 출력하는 모델을 분리해야한다. 일반적으로 입력 데이터를 저차원 데이터로 압축하는 모델을 _인코더_, 압축된 데이터에서 고차원 데이터로 차원을 확장하는 모델을 _디코더_ 모델이라 하고, 인코더 디코더를 같이 쓰면 인코더 디코더 모델이라 한다.

## 2.1 다대일 모델
![](https://i.imgur.com/3SIMb22.png)

다대일 모델은 입력은 순차열이지만, 출력은 순차열이 아닐때 사용하며, 모든 단계에서 입력을 받지만, 출력은 마지막 단계에서만 한다. 보통 감성 분석을 할 때 다대일모델을 사용한다.

### 다대다 모델
![](https://i.imgur.com/pbKUTV4.png)

다대다 모델은 입출력의 길이가 같은 순차열일때 사용하며 모든 단계에서 입력을 받고, 모든 단계에서 출력한다. 

### 티처 포싱
다대다 모델에서 티처 포싱 방식으로 학습하면 학습이 안정화되고 수렴 속도도 빨라진다.

티처 포싱은 현재 단계의 출력을 다음 단계에 입력하는 방법이다. 현재 단계의 예측 결과를 교사의 지도 신호로 사용해서 다음 단계에 전달하면 예측 성능이 높아질 수 있다.

티처 포싱을 사용할 때는 주의해야할 점이 하나 있다. 훈련시에 모델의 예측 결과대신 타깃을 직접 지도 신호로 사용해야한다. 훈련중에는 모델이 정확한 예측을 할 수 없으므로 모델의 예측을 지도 신호로 사용하기엔 부적절하다.


## 2.3 일대다 모델
일대다 모델은 입력은 순차열이 아니지만 출력은 순차열일 때 사용한다. 보통 이미지 캡션을 생성할 때 사용한다.

## 2.4 양방향 모델
양방향모델은 입력을 양쪽으로 보는 것이다. 시간순으로 생성되는 데이터는 인과 관계에 따라 현재는 과거에만 의존하기 때문에 시간의 흐름 방향으로만 봐야 하겠지만, 공간적 순서 관계를 갖는 데이터는 상대적인 순서가 중요하므로 양방향으로 살펴보고 판단하는 것이 더 정확하다.

## 2.5 인코더 디코더 모델
입력과 출력의 길이가 서로 다른 순차열일때 사용하며, 입력데이터를 요약하는 인코더와 요약 데이터를 이용해서 출력 데이터를 출력한느 디코더로 구성된다.

Seq2Seq 모델이라고도 부른다.

인코드는 입력데이터를 순차적으로 처리하다가 마지막 단계의 은닉상태를 콘텍스트 벡터로 출력한다. 

# 3. 시간펼침 역전파
순환 신경망에는 역전파 알고리즘을 어떻게 적용해야할까? 순환 신경망을 시간 순서대로 펼쳐놓으면 입력에서 출력까지 뉴런의 실행순서가 정해지는 만큼, 정확히 반대 순서로 역전파를 수행하면 된다. 

이를 시간펼침 역전파로 구분하며 간단히 _BPTT_ 라 부른다.

## 3.1 순환 신경망의 손실함수
손실함수 또한 시간 순서대로 펼쳐놓은 상태에서 정의해야 한다. 단계별로 출력하기 때문에 손실함수도 단계별로 정의된다.

순환신경망의 전체 손실 함수는 모든 단계의 손실함수를 더해서 정의한다.

## 3.2 BPTT
### 오차의 역전파 범위와 순서
마지막 단계의 출력 
# 4. LSTM과 GRU 
> [!note]+ IDEA
> 기본 순환 신경망은 최적화하기 어렵고, 성능적인 한계도 존재한다.
> 이런 점들을 극복하고자 LSTM과 GRU와 같은 셀 구조를 갖는 순환 신경망이 등장했다.

## 4.1 기본 순환 신경망의 문제점
기본 순환 신경망은 시간이 지나면서 입력 데이터의 영향이 사라지는 __장기 의존성 문제__ 와 __그레디언트 소실과 폭발__ 이 쉽게 일어나는 구조적 문제가 있다.

### 장기의존성
_장기의존성(Long-term dependency)_ 문제는 콘텍스트의 범위가 넓을 때 멀어진 입력에 대한 의존성이 있음에도 불구하고 입력의 영향이 점점 사라지는 현상을 말한다.

 이 경우에는 순차열이 길어질수록 오래전에 입력된 데이터의 정보는 사라지기 때문에 정확한 예측을 할 수 없다. 이 문제를 해결하기 위해서는 콘텍스트가 오래 지속하도록 구조를 변경해야 한다.
### 그래디언트 소실과 폭발
_그래디언트 소실과 폭발(Gradient vanishing and exploding)_ 문제는 학습하면서 그래디언트가 없어지거나 발산하는 현상을 말한다.

행렬이 반복적으로 곱해지면서 거듭제곱이 되므로, 각 차원의 고윳값 크기가 1보다 크면 발산하고 크기가 1보다 작으면 0으로 수렴한다. 기본순환신경망은 가중치 행렬W가 반복적으로 곱해지는 구조때문에 그래디언트 소실과 폭발이 쉽게 일어난다.

### 그래디언트 클리핑
그래디언트 폭발은 _그래디언트 클리핑_ 을 해서 일정 크기 이상으로 커지지 않게 하면 비교적 쉽게 막을 수 있다. 

그레디언트가 $g$고 임계치가 $v$라고 할 때, $g$의 크기가 $v$보다 크다면 $g$를 $g\over{||g||}$와 같이 단위 벡터로 만든 뒤에 $v$를 곱하면  $v$와 같은 크기의 벡터가 된다.

순환 신경망의 손실 곡면에는 가파른 절벽이 많은데, 가파른 절벽이 큰 손실 곡면에서는 최적화하기 어렵다.(엉뚱한 방향, 경로 이탈 -> 늦어지거나 안되는 경우 발생)

그레디언트 클리핑을 하면 보폭이 줄어들어 경로이탈이 잘 일어나지 않고 조금씩 전진한다.

## 4.2 LSTM
>[!note]+
>LSTM은 그레디언트 소실의 원인이 되는 가중치 W와의 행렬곱 연산이 그레디언트 경로에 나타나지 않도록 구조를 변경한 모델이다.

### 그레디언트 소실을 막는 모델 구조
LSTM은 셀 구조로 되어있어 셀에는 기본 순환 신경망에 있던 은닉상태 $h_t$외에 셀 상태 $C_t$가 추가되었다.


![](https://i.imgur.com/zWip7km.png)

셀 상태를 연결하는 경로에는 가중치 W와의 행렬곱 연산이 없다. 그에 따라 순방향 흐름과 역방향 흐름이 원활해지면서 장기 의존성 문제와 그레디언트 소실 문제가 완화된다.

셀 사이에 정보를 원활히 전달하고 그레디언트 소실도 막을 수 있도록 별도의 경로를 두는 설계 방식은 레즈넷이나 덴스넷의 잔차 연결 방식과 유사하다.


###  LSTM 구조
LSTM의 핵심은 Cell state이다. 

#### 모듈요소
![](https://i.imgur.com/YMukQih.png)
- 노란 박스는 학습된 레이어를 뜻함
- 분홍색 동그라미는 vector합과 같은 pointwise 연산을 뜻함

#### Cell state
![](https://i.imgur.com/LXJJXDg.png)
- 모듈 그림에서 수평으로 그어진 윗선에 해당
- 메모리와 같은 존재이며, 컨베이어 벨트와 같아서 State가 꽤 오래 경과하더라도 gradient가 잘 전파된다.
- Gate라 불리는 구조에 의해 정보가 추가되거나 제거된다.
- Gate는 학습을 통해 어떤 정보를 유지하고 어떤 정보를 버릴 지 학습한다.

#### gate
![](https://i.imgur.com/8DQ9Mba.png)

- LSTM은 3개의 Gate를 가지고 있고, 이 게이트들은 셀 상태를 보호하고 제어한다.
> [!page]+
> - 망각게이트 : 과거 정보를 잊기 위한 게이트
> - 입력게이트 : 현재 정볼르 기억하기 위한 게이트
> - 출력 게이트 : 최종 결과를 내보내기 위한 게이트

- 3개의 게이트에는 공통적으로 시그모이드 함수가 존재하는데, 시그모이드를 지나면서 0과1사이의 값이 나오게 되는데 이 값들을 가지고 게이트를 조절한다.

