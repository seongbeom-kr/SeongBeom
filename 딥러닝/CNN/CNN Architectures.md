#deep_learning #CNN #Architecture 


# 1. LeNet - 5
> 최초의 컨볼루션 신경망
![[스크린샷 2024-05-27 02.33.09.png]]
## 1.1 모델구조
르넷-5는 `CONV - POOL - CONV - POOL - FC - FC - FC`로 구성된 7계층 모델이다.

![[Pasted image 20240527023538.png]]

컨볼루션 계층과 풀링 계층을 2번 씩 반복한 뒤 완전 연결 3계층으로 연결되는 구조다.

컨볼루션 계층과 풀링 계층은 생체 신경망의 시각 영역을, 완전연결계층은 연관영역을 모델링 했다.


# 2. AlexNet
`이미지넷(imageNet)` 은 약 1,400만 개의 이미지로 구성된 이미지 데이터 베이스로서 20000만여개의 카테고리를 포함한다.

그중 알렉스넷은 이미지넷을 활용한 객체인식오류를 압도적으로 낮추며 1위를 차지한 최초의 컨볼루션 신경망 모델이다.

## 2.1 모델 구조
알렉스 넷은 2개의 GPU에서 실행되는 병렬처리 구조로 설계되었다. 

~~당시 gpu로는 한개로 실행시킬 수 없어 병렬처리 구조로 설계하고 2개의 gpu에서 실행~~

알렉스 넷은 컨볼루션 필터를 두 그룹으로 나눈 뒤 그룹별로 GPU를 할당하여 처리하고, 중간 계층에서 정보를 교환하여 최종 결과를 한쪽 그룹으로 합친다.

### 병렬처리 방식
첫 번째 계층에 $277 \times 277 \times 3$ 입력 데이터가 들어오면 그룹별로 48개 컨볼루션 필터가 연산을 수행한다.

정보를 교환하기 위해 Conv3과 Conv5, FC6, FC7 계층에서 그룹 간 출력을 교환하여 출력 계층인 FC8에서 전체 결과를 합친다.

![[스크린샷 2024-05-27 02.43.31.png]]

이미넷으로 알렉스 넷을 훈련했을 때 5~6일 정도 소요되었고, 이들이 사용한 그룹을 나눠 처리하는 방식은 그룹 컨볼루션이란 이름으로 레즈넥스트 모델에 도입되어 그룹 내에서 채널 간의 상관관계 구조를 학습하는 용도로 활용되고 있다.

### 모델구조
알렉스넷의 병렬처리 구조를 하나의 구조로 합치면 다음과 같다.

알렉스 넷은 `conv-conv-conv-conv-conv-fc-fc-fc`로 구성된 8계층 모델이다.

컨볼루션 5계층이 완전연결 3계층과 연결된 구조이다. (이때 풀링은 컨볼루션 계층에 포함된다.)

알렉스넷의 첫 번째 계층은 `11x11, 4` 컨볼루션 연산을 진행한다. 

상대적으로 큰 컨볼루션 피러와 스트라이드를 사용하므로 학습이 잘 안되는 문제가 존재하며, 나머지 컨볼루션 계층은 `3x3, 1` 컨볼루션 연산을 한다.

CONV1, CONV2, CONV5에서는 `3x3, 2` 로 맥스 풀링하며 한 픽셀씩 겹치도록 풀링한다.

CONV1과 CONV2에서는 지역응답정규화라고 불리는 _LRN(local response normalization)_ 방식으로 출력을 정규화한다.

LRN정규화는 픽셀별로 이웃 채널의 픽셀을 이용하여 정규화하는 방식으로 지금은 잘 사용하지 않는다.

ReLU의 출력이 무한히 커질 수 있으므로 이를 막기 위한 정규화로 도입되었다. 227x227x3 입력이 들어왔을 때, 첫 번째 계층은 컨볼루션 맥스풀링 정규화 순서로 실행하며, 드랍아웃은 FC6, FC7계층에만 적용되어 있다.

### 모델 파라미터 수
알렉스 넷의 파라미터 수는 6천200백만 개에 달한다.

알렉스넷의 파라미터는 FC계층에 집중되어있는데, 초기 컨볼루션 신경망 모델들은 생체 신경망의 영역의 연관영역을 모델링하기 위해 FC계층을 포함하지만, 그 결과 파라미터가 과도하게 쓰이는 문제가 존재한다. 

구글넷 이후 나온 모델들은 파라미터를 줄이기 위해 출력 계층을 제외하고 모든 계층을 컨볼루션계층으로 구성한다.

## 2.2 훈련방식
- GPU를 활용함으로써 압도적인 성능 향상
- LRN, 드롭아웃(p = 0.5), $L_2$정규화, 데이터 증강 등 정규화 기법사용
- 
