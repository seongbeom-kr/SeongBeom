> [!note] 이전 요약
> 퍼셉트론을 통해서 우리는 다양한 작업을 진행할 수 있다. 
>
>하지만 단점이 존재하는데, 이는 가중치와 편향을 사용자가 수동으로 조절해서 최적의 퍼셉트론을 만들어야 한다는 점이다.
>
>이를 해결하기 위해 퍼셉트론에서 신경망으로 나아간다.
## 1. 퍼셉트론에서 신경망으로
### 1.1 신경망의 예시

![400](https://i.imgur.com/90UVYIM.png)

그림으로만 봤을 때는 다층 퍼셉트론하고 구분이 안간다.

이를 구분하기 위해 뉴런(노드) 안 구조를 살펴봐야 한다.

### 1.2 퍼셉트론 복습
다음과 같은 그림과 수식을 예시로 사용한다.

![](https://i.imgur.com/AH1iLbB.png)

가중치와 편향은 매개변수이며, 편향은 뉴런이 얼마나 쉽게 활성화 될지, 가중치는 각 노드(신호)의 영향력을 제어하는 역할을 한다.

하지만 그림에서는 편향이 나타나지않는데, 이를 그림으로 표현하면 다음과 같다.


![400](https://i.imgur.com/v8ypcQJ.png)

위 그림에서는 입력이 1이고 가중치가 b인 노드가  추가된 것을 볼 수 있다. 이 동작을 살펴보면 모든 퍼셉트론에는 여러개의 신호와 1이라는 신호로 구성되어 있다는 것을 알 수 있다.

이때 편향의 입력신호는 항상 1이기 때문에 그림에 추가안하거나, 회색으로 표기하기도 함

이를 수식으로 간단하게 표현하면 다음과 같다.

$$y = h(b+w_1x_1+w_2x_2) \tag{3.1}$$ 
$$
h(x) = 
\begin{cases}
0 \; (x\leq0)\\
1 \; (x\gt0) 
\end{cases} \tag{3.2}$$

3.1 식은 입력신호의 총합이 $h(x)$라는 함수를 거쳐 변환되어, 그 변환된 값이 y의 출력이 된다는 것을 보여주고,

3.2 식은 $h(x)$ 함수는 입력이 0을 넘으면 1을 돌려주고, 그렇지 않으면 0을 돌려준다.

즉, 맨 위의 식과 두개의 식은 같은 역할을 한다는 것을 알 수 있다.

### 1.3 활성화 함수의 등장
위에서 $h(x)$함수가 등장했는데, 이처럼 입력 신호의 총합을 출력신호로 변환하는 함수를 일반적으로 **활성화함수(activation funmction)** 이라 한다

3.1 식은 가중치가 곱해진 입력신호의 총합을 계산하고, 그 합을 활성화함수에 입력해 결과를내는 2단계로 처리된다.

그래서 두개의 식으로 나눌 수 있다.

$$a = b+w_1x_1+w_2x_2 \tag{3.3}$$
$$y = h(a)  \tag{3.4}$$

3.3 식은 가중치가 달린 입력신호화 편향의 총합을 계산하고, 이를 a라고 한다.

3.4 식은 a를 $h()$에 넣어 y를 출력하는 흐름이다.

이를 도식화하면 다음과 같다.

![400](https://i.imgur.com/YlrZXPO.png)

$h()$가 활성화함수다.

## 2. 활성화함수
3.2 식과 같은 활성화함수는 임계값을 경계로 출력이 바뀐다. 이런 함수를 계단함수(step function)이라 한다

활성화 함수에는 여러 종류의 활성화 함수가 존재한다.

### 2.1 시그모이드 함수
자주 이용하는 활성화 함수는 시그모이드 함수이며, 다음과 같은 수식으로 표현한다.
$$
h(x) = \frac{1}{1 + e^{-x}} \tag{3.5}
$$

exp(-x)는 $e^{-x}$를 뜻하며 e는 자연상수로 2.7182의 값을 갖는 실수이다.

그냥 함수이니 어려워 하지 말자(계산하라는 소리 안함)

### 2.2 계단 함수

![350](https://i.imgur.com/juFVyys.png)

이 그래프에서는 0을 경계로 출력이 0 또는 1값을 가진다. 진짜 계단처럼 생겨서 이름 지음(단순)

### 2.3 시그모이드 함수
