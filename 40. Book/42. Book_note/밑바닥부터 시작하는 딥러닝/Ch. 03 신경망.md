> [!note] 이전 요약
> 퍼셉트론을 통해서 우리는 다양한 작업을 진행할 수 있다. 
>
>하지만 단점이 존재하는데, 이는 가중치와 편향을 사용자가 수동으로 조절해서 최적의 퍼셉트론을 만들어야 한다는 점이다.
>
>이를 해결하기 위해 퍼셉트론에서 신경망으로 나아간다.
## 1. 퍼셉트론에서 신경망으로
### 1.1 신경망의 예시

![400](https://i.imgur.com/90UVYIM.png)

그림으로만 봤을 때는 다층 퍼셉트론하고 구분이 안간다.

이를 구분하기 위해 뉴런(노드) 안 구조를 살펴봐야 한다.

### 1.2 퍼셉트론 복습
다음과 같은 그림과 수식을 예시로 사용한다.

![](https://i.imgur.com/AH1iLbB.png)

가중치와 편향은 매개변수이며, 편향은 뉴런이 얼마나 쉽게 활성화 될지, 가중치는 각 노드(신호)의 영향력을 제어하는 역할을 한다.

하지만 그림에서는 편향이 나타나지않는데, 이를 그림으로 표현하면 다음과 같다.


![400](https://i.imgur.com/v8ypcQJ.png)

위 그림에서는 입력이 1이고 가중치가 b인 노드가  추가된 것을 볼 수 있다. 이 동작을 살펴보면 모든 퍼셉트론에는 여러개의 신호와 1이라는 신호로 구성되어 있다는 것을 알 수 있다.

이때 편향의 입력신호는 항상 1이기 때문에 그림에 추가안하거나, 회색으로 표기하기도 함

이를 수식으로 간단하게 표현하면 다음과 같다.

$$y = h(b+w_1x_1+w_2x_2) \tag{3.1}$$ 
$$
h(x) = 
\begin{cases}
0 \; (x\leq0)\\
1 \; (x\gt0) 
\end{cases} \tag{3.2}$$

3.1 식은 입력신호의 총합이 $h(x)$라는 함수를 거쳐 변환되어, 그 변환된 값이 y의 출력이 된다는 것을 보여주고,

3.2 식은 $h(x)$ 함수는 입력이 0을 넘으면 1을 돌려주고, 그렇지 않으면 0을 돌려준다.

즉, 맨 위의 식과 두개의 식은 같은 역할을 한다는 것을 알 수 있다.

### 1.3 활성화 함수의 등장
위에서 $h(x)$함수가 등장했는데, 이처럼 입력 신호의 총합을 출력신호로 변환하는 함수를 일반적으로 **활성화함수(activation funmction)** 이라 한다

3.1 식은 가중치가 곱해진 입력신호의 총합을 계산하고, 그 합을 활성화함수에 입력해 결과를내는 2단계로 처리된다.

그래서 두개의 식으로 나눌 수 있다.

$$a = b+w_1x_1+w_2x_2 \tag{3.3}$$
$$y = h(a)  \tag{3.4}$$

3.3 식은 가중치가 달린 입력신호화 편향의 총합을 계산하고, 이를 a라고 한다.

3.4 식은 a를 $h()$에 넣어 y를 출력하는 흐름이다.

이를 도식화하면 다음과 같다.

![400](https://i.imgur.com/YlrZXPO.png)

$h()$가 활성화함수다.

## 2. 활성화함수
3.2 식과 같은 활성화함수는 임계값을 경계로 출력이 바뀐다. 이런 함수를 계단함수(step function)이라 한다

활성화 함수에는 여러 종류의 활성화 함수가 존재한다.

### 2.1 시그모이드 함수
자주 이용하는 활성화 함수는 시그모이드 함수이며, 다음과 같은 수식으로 표현한다.
$$
h(x) = \frac{1}{1 + e^{-x}} \tag{3.5}
$$

exp(-x)는 $e^{-x}$를 뜻하며 e는 자연상수로 2.7182의 값을 갖는 실수이다.

그냥 함수이니 어려워 하지 말자(계산하라는 소리 안함)

### 2.2 계단 함수

![350](https://i.imgur.com/juFVyys.png)

이 그래프에서는 0을 경계로 출력이 0 또는 1값을 가진다. 진짜 계단처럼 생겨서 이름 지음(단순)

### 2.3 시그모이드 함수

![400](https://i.imgur.com/F0hZ93i.png)

시그모이드는 S자 모양이다. 즉 경계선이 매끄럽다

### 2.4 비교
1. 시그모이드는 부드로운 곡선이며, 입력에 따라 출력이 연속적으로 변화한다. 하지만 계단함수는 0을 경계로 출력이 바뀐다.
	1. 이 매끈함의 차이가 매우 중요함 (미분 가능함을 나타내거든)
2. 둘은 그래도 0~1의 값을 출력한다는 점에서 비슷함

### 2.5 비선형 함수
두 함수는 둘다 비선형함수라는 특징을 가진다.

>[!note] 비선형함수
>함수를 사용했을 때 출력이 입력의 상수배만큼 변하는 함수를 선형함수라 함
>
>비선형 함수는 말 그대로 선형이 아닌 함수, 직선 1개로는 그릴 수 없는 함수를 말한다.(곡선, S자, 계단식 등)

근데 왜 선형함수를 안쓸까? 

우리는 이제 층을 깊게 쌓을텐데 선형함수를 사용하면 층을 쌓는 이유가 없어진다.(affine 함수) 

선형함수를 통해 깊게 쌓으면 돌고돌아 선형함수이기 때문이다. 

먼저, 선형함수의 정의는 다음과 같다고 가정

$$
f(x) = Wx + b
$$

여기서 W는 가중치(weight), b는 편향(bias), x는 입력 벡터

이제 두 개의 선형함수를 연속으로 적용하면 다음과 같다.

첫 번째 층:

$$
h_1 = W_1x + b_1
$$

두 번째 층:

$$
h_2 = W_2h_1 + b_2 = W_2(W_1x + b_1) + b_2
$$

이를 확장하면:

$$
h_2 = W_2W_1x + W_2b_1 + b_2
$$

이것은 여전히 선형 함수입니다. 즉, h_2는 x에 대해 선형적이다.

이를 일반화하면, L개의 선형 층을 쌓아도 결과는 여전히 선형 함수임을 알 수 있다. :

$$
h_L = W_L \cdots W_2W_1x + \sum_{i=1}^{L} W_i b_{i-1}
$$

여기서 $b_0 = x$ 입니다. 따라서 다층 선형 모델은 본질적으로 단일 선형 모델과 동일한 표현력을 가진다.

반면 비선형 활성화 함수  $\phi$를 사용하면 다음과 같이 각 층에서 비선형성이 도입된다.

첫 번째 층:

$$
h_1 = \phi(W_1x + b_1)
$$

두 번째 층:

$$
h_2 = \phi(W_2h_1 + b_2)
$$


### 2.7 ReLU 함수
요즘에 많이 쓰이는 함수는 **ReLU**임 (렐루~)


![](https://i.imgur.com/ObZaEwV.png)
수식으로 표현하면 다음과 같다.

$$h(x) = \begin{cases} x \; (x\gt0) \\ 0 \; (x\leq0)$$